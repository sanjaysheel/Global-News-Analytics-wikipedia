{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0e0c8f-b9a2-4149-a790-67cd0d103ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing library or modules# \n",
    "import os\n",
    "import json\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc3697e-b5fc-42d3-abfb-18ec51a0a0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the DBFS path to the file\n",
    "file_path = \"/Workspace/Cred/secrets.ipynb\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File {file_path} does not exist.\")\n",
    "else:\n",
    "    # # Read the content of the file\n",
    "    # with open(file_path, 'r') as file:\n",
    "    #     file_content = file.read()\n",
    "\n",
    "    # # Display the content of the file\n",
    "    # print(file_content)\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        notebook_content = file.read()\n",
    "\n",
    "    # Parse the JSON content\n",
    "    notebook_json = json.loads(notebook_content)\n",
    "    # aws_access_key = notebook_json['aws_access_key']\n",
    "    # aws_secret_access_key = notebook_json['aws_secret_access_key']\n",
    "        \n",
    "    # Extract the source lines from the first cell\n",
    "    source_lines = notebook_json['cells'][0]['source']\n",
    "\n",
    "    # Combine lines into a single string\n",
    "    code_str = ''.join(source_lines)\n",
    "\n",
    "    # Now you can parse the string to extract the keys using regex\n",
    "    import re\n",
    "\n",
    "    access_key_match = re.search(r'aws_access_key\\s*=\\s*\"([^\"]+)\"', code_str)\n",
    "    secret_key_match = re.search(r'aws_secret_key\\s*=\\s*\"([^\"]+)\"', code_str)\n",
    "\n",
    "    aws_access_key = access_key_match.group(1) if access_key_match else None\n",
    "    aws_secret_key = secret_key_match.group(1) if secret_key_match else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee57923b-0e62-40ee-bf10-3c8941ee663e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create_session\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    region_name='us-west-2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491b4964-4e47-4690-8807-4377178a62e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s3 = session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f9303c-b5f7-4175-a219-aab91b6a2122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bucket_name = 'ind-wiki-scrape-1'  # valid bucket name (lowercase, hyphens)\n",
    "\n",
    "\n",
    "\n",
    "# For us-east-1, no CreateBucketConfiguration is needed\n",
    "if session.region_name == 'us-east-1':\n",
    "    # Check if the bucket exists\n",
    "    existing_buckets = s3.list_buckets()\n",
    "    if not any(bucket['Name'] == bucket_name for bucket in existing_buckets['Buckets']):\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "else:\n",
    "    # Check if the bucket exists\n",
    "    existing_buckets = s3.list_buckets()\n",
    "    if not any(bucket['Name'] == bucket_name for bucket in existing_buckets['Buckets']):\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': session.region_name}\n",
    "        )\n",
    "\n",
    "\n",
    "response = s3.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "aws_connect",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
